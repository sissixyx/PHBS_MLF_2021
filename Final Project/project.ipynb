{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\compat\\_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.8' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps,\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:597: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:836: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1097: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1344: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1480: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:152: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  precompute=False, eps=np.finfo(np.float).eps,\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:320: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, random_state=None,\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:580: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=4 * np.finfo(np.float).eps, n_jobs=None,\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py:34: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from ._gradient_boosting import predict_stages\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py:34: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from ._gradient_boosting import predict_stages\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:31: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import sklearn.preprocessing as skpre\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from functools import reduce\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_wine\n",
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import  make_scorer\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "def Dummy(yields,a=.02,b=-.02):  #change raise to state\n",
    "    dummy=[]\n",
    "    for i in range(len(yields)):\n",
    "        if yields.iloc[i]>=a:\n",
    "            dummy.append(1)\n",
    "        elif yields.iloc[i]<b:\n",
    "            dummy.append(-1)\n",
    "        else:\n",
    "            dummy.append(0)\n",
    "    output=pd.DataFrame(dummy)\n",
    "    return output\n",
    "def metrics(C):   #output rates\n",
    "    print('TrueShortRate=',C[0][0]/C[0].sum())\n",
    "    print('FalseShortRate=',C[0][2]/C[0].sum())\n",
    "    print('CaughtShortRate=',C[0][0]/C.iloc[0,:].sum())\n",
    "    print('TrueLongRate=',C[2][2]/C[2].sum())\n",
    "    print('FalseLongRate=',C[2][0]/C[2].sum())\n",
    "    print('CaughtLongRate=',C[2][2]/C.iloc[2,:].sum())\n",
    "\n",
    "def add(x,y):\n",
    "    return(1+x)*(1+y)\n",
    "\n",
    "def yields(predict,returns): #output yileds\n",
    "    real=returns.iloc[-len(predict):]\n",
    "    if len(predict)==len(real):\n",
    "        yields=[]\n",
    "        for i in range(len(predict)):\n",
    "            if int(predict.iloc[i])==1:\n",
    "                yields.append(real.iloc[i]) \n",
    "            elif int(predict.iloc[i])==-1:\n",
    "                yields.append(-real.iloc[i])\n",
    "            else:\n",
    "                yields.append(0)\n",
    "#        accy=pd.DataFrame(reduce(add,yields))\n",
    "            for i in range(len(yields)):\n",
    "                if yields[i]<=-1:\n",
    "                    return '爆仓'\n",
    "                    \n",
    "        yields=pd.DataFrame(yields)    \n",
    "        return yields\n",
    "        pring('wrong input')\n",
    "def accy(yields)  : #ouput accumulated yields\n",
    "    if type(yields)==str:\n",
    "        return '爆仓'\n",
    "    else:\n",
    "        accy=[]\n",
    "        for i in range(len(yields)):\n",
    "                if i==0:\n",
    "                    accy.append(1+yields.iloc[0])\n",
    "                else:\n",
    "                    accy.append((accy[i-1])*(1+yields.iloc[i]))\n",
    "        return pd.DataFrame(accy)\n",
    "def my_score(estimator,X_train,y_train,X_test,y_test):    \n",
    "    model=estimator.fit(X_train,y_train)\n",
    "    C=pd.DataFrame(confusion_matrix(y_test,pd.DataFrame(estimator.predict(X_test)) ,labels=[-1,0,1])) \n",
    "    if C[0].sum()!=0:\n",
    "        TrueShortRate=C[0][0]/C[0].sum()\n",
    "        FalseShortRate=C[0][2]/C[0].sum()\n",
    "        CaughtShortRate=C[0][0]/C.iloc[0,:].sum()\n",
    "    else:\n",
    "        return -10000\n",
    "    if C[2].sum()!=0:\n",
    "        TrueLongRate=C[2][2]/C[2].sum()\n",
    "        FalseLongRate=C[2][0]/C[2].sum()\n",
    "        CaughtLongRate=C[2][2]/C.iloc[2,:].sum()\n",
    "    else:\n",
    "        return -10000\n",
    "    if TrueShortRate-FalseShortRate<0 and TrueLongRate-FalseLongRate<0:\n",
    "        return -10000\n",
    "    score=(TrueShortRate-FalseShortRate)*(TrueLongRate-FalseLongRate)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Input there any NaN? False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Inv_Steel_pct</th>\n",
       "      <th>Inv_Iron_pct</th>\n",
       "      <th>GP_Rebar_pct</th>\n",
       "      <th>Cost_Rebar_pct</th>\n",
       "      <th>SP_Rebar_pct</th>\n",
       "      <th>SP_Iron_pct</th>\n",
       "      <th>FP_FS_pct</th>\n",
       "      <th>FB_Mn-Si_pct</th>\n",
       "      <th>FB_Coke_pct</th>\n",
       "      <th>SP_Coke_pct</th>\n",
       "      <th>...</th>\n",
       "      <th>M2</th>\n",
       "      <th>PPI</th>\n",
       "      <th>CA_REDI</th>\n",
       "      <th>CA_FAI</th>\n",
       "      <th>HCA</th>\n",
       "      <th>CSA</th>\n",
       "      <th>CHSA</th>\n",
       "      <th>PPI_I_yoy</th>\n",
       "      <th>PPI_I_mom</th>\n",
       "      <th>FP_Rebar.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.024660</td>\n",
       "      <td>0.004041</td>\n",
       "      <td>0.102218</td>\n",
       "      <td>0.038983</td>\n",
       "      <td>0.007880</td>\n",
       "      <td>0.003738</td>\n",
       "      <td>-0.018208</td>\n",
       "      <td>-0.010855</td>\n",
       "      <td>-0.193654</td>\n",
       "      <td>-0.000485</td>\n",
       "      <td>...</td>\n",
       "      <td>1519485.40</td>\n",
       "      <td>0.83</td>\n",
       "      <td>9377.0233</td>\n",
       "      <td>57522.5767</td>\n",
       "      <td>14951.9293</td>\n",
       "      <td>1.3</td>\n",
       "      <td>15153.7476</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>3039.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.006727</td>\n",
       "      <td>0.004293</td>\n",
       "      <td>0.035225</td>\n",
       "      <td>0.039732</td>\n",
       "      <td>0.063727</td>\n",
       "      <td>0.026071</td>\n",
       "      <td>-0.001455</td>\n",
       "      <td>-0.010507</td>\n",
       "      <td>0.020255</td>\n",
       "      <td>-0.005820</td>\n",
       "      <td>...</td>\n",
       "      <td>1530432.06</td>\n",
       "      <td>2.22</td>\n",
       "      <td>9412.4679</td>\n",
       "      <td>54118.9623</td>\n",
       "      <td>14140.9447</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>15491.2065</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3198.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.016677</td>\n",
       "      <td>-0.012378</td>\n",
       "      <td>-0.038519</td>\n",
       "      <td>0.079217</td>\n",
       "      <td>0.035875</td>\n",
       "      <td>-0.005445</td>\n",
       "      <td>-0.008012</td>\n",
       "      <td>-0.110665</td>\n",
       "      <td>-0.007657</td>\n",
       "      <td>-0.018537</td>\n",
       "      <td>...</td>\n",
       "      <td>1530432.06</td>\n",
       "      <td>2.22</td>\n",
       "      <td>9412.4679</td>\n",
       "      <td>54118.9623</td>\n",
       "      <td>14140.9447</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>15491.2065</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3251.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.029112</td>\n",
       "      <td>0.024795</td>\n",
       "      <td>-0.120141</td>\n",
       "      <td>-0.018459</td>\n",
       "      <td>0.034632</td>\n",
       "      <td>-0.034672</td>\n",
       "      <td>-0.031938</td>\n",
       "      <td>-0.011674</td>\n",
       "      <td>-0.059160</td>\n",
       "      <td>-0.014911</td>\n",
       "      <td>...</td>\n",
       "      <td>1530432.06</td>\n",
       "      <td>2.22</td>\n",
       "      <td>9412.4679</td>\n",
       "      <td>54118.9623</td>\n",
       "      <td>14140.9447</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>15491.2065</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3088.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.035524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.398391</td>\n",
       "      <td>0.036036</td>\n",
       "      <td>-0.058050</td>\n",
       "      <td>-0.003781</td>\n",
       "      <td>-0.032992</td>\n",
       "      <td>-0.089933</td>\n",
       "      <td>-0.077764</td>\n",
       "      <td>-0.016145</td>\n",
       "      <td>...</td>\n",
       "      <td>1530432.06</td>\n",
       "      <td>2.22</td>\n",
       "      <td>9412.4679</td>\n",
       "      <td>54118.9623</td>\n",
       "      <td>14140.9447</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>15491.2065</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2923.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Inv_Steel_pct  Inv_Iron_pct   GP_Rebar_pct  Cost_Rebar_pct  SP_Rebar_pct  \\\n",
       "0       0.024660      0.004041       0.102218        0.038983      0.007880   \n",
       "1       0.006727      0.004293       0.035225        0.039732      0.063727   \n",
       "2       0.016677     -0.012378      -0.038519        0.079217      0.035875   \n",
       "3       0.029112      0.024795      -0.120141       -0.018459      0.034632   \n",
       "4       0.035524      0.000000      -0.398391        0.036036     -0.058050   \n",
       "\n",
       "   SP_Iron_pct  FP_FS_pct  FB_Mn-Si_pct  FB_Coke_pct  SP_Coke_pct  ...  \\\n",
       "0     0.003738  -0.018208     -0.010855    -0.193654    -0.000485  ...   \n",
       "1     0.026071  -0.001455     -0.010507     0.020255    -0.005820  ...   \n",
       "2    -0.005445  -0.008012     -0.110665    -0.007657    -0.018537  ...   \n",
       "3    -0.034672  -0.031938     -0.011674    -0.059160    -0.014911  ...   \n",
       "4    -0.003781  -0.032992     -0.089933    -0.077764    -0.016145  ...   \n",
       "\n",
       "           M2   PPI    CA_REDI      CA_FAI         HCA  CSA        CHSA  \\\n",
       "0  1519485.40  0.83  9377.0233  57522.5767  14951.9293  1.3  15153.7476   \n",
       "1  1530432.06  2.22  9412.4679  54118.9623  14140.9447 -0.5  15491.2065   \n",
       "2  1530432.06  2.22  9412.4679  54118.9623  14140.9447 -0.5  15491.2065   \n",
       "3  1530432.06  2.22  9412.4679  54118.9623  14140.9447 -0.5  15491.2065   \n",
       "4  1530432.06  2.22  9412.4679  54118.9623  14140.9447 -0.5  15491.2065   \n",
       "\n",
       "   PPI_I_yoy  PPI_I_mom  FP_Rebar.1  \n",
       "0        1.2        0.7      3039.0  \n",
       "1        3.3        1.5      3198.0  \n",
       "2        3.3        1.5      3251.0  \n",
       "3        3.3        1.5      3088.0  \n",
       "4        3.3        1.5      2923.0  \n",
       "\n",
       "[5 rows x 72 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data\n",
    "weekdata=pd.read_excel(r\"D:\\Enter\\学习资料\\Machine Learning in Finance\\Project\\weekly data-EN.xlsx\",sheet_name='Sheet1')\n",
    "weekdata=weekdata.iloc[:,1:].interpolate()  #Fill NaN\n",
    "\n",
    "Input1=weekdata.pct_change().drop(index=[0,len(weekdata)-1]).reset_index(drop=1)      \n",
    "Input2=weekdata.drop(index=[0,len(weekdata)-1]).reset_index(drop=1)   \n",
    "Input=Input1.join(Input2,lsuffix='_pct').drop(columns=['PPI_I_yoy_pct','PPI_I_mom_pct'])\n",
    "price=pd.read_excel(r\"D:\\Enter\\学习资料\\Machine Learning in Finance\\Project\\yields.xlsx\",sheet_name='Sheet1').drop('Time',axis=1).drop(index=[0,1]).reset_index(drop=1)  \n",
    "y=Dummy(price.iloc[:,1])\n",
    "yr=pd.DataFrame(price.iloc[:,1])\n",
    "print(\"Is Input there any NaN?\",Input.isnull().values.any()) \n",
    "#Split Training and Test\n",
    "X_train=Input.iloc[0:int(0.8*len(Input)),:]\n",
    "X_test=Input.iloc[int(0.8*len(Input)):,:]\n",
    "y_train=y.iloc[0:int(0.8*len(y)),:]\n",
    "y_test=y.iloc[int(0.8*len(y)):,:]\n",
    "yr_test=yr.iloc[int(0.8*len(y)):,:]\n",
    "#X_train, X_test, y_train, y_test =\\\n",
    "#    train_test_split(Input, y, test_size=0.2, random_state=0, stratify=None)\n",
    "tscv = TimeSeriesSplit(n_splits=5,max_train_size=50)\n",
    "Input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of      0\n",
       "0    1\n",
       "1    0\n",
       "2   -1\n",
       "3   -1\n",
       "4    0\n",
       "..  ..\n",
       "248  0\n",
       "249  0\n",
       "250  0\n",
       "251 -1\n",
       "252 -1\n",
       "\n",
       "[253 rows x 1 columns]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.052320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.016573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.050138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.053433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.012658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Return\n",
       "0  0.052320\n",
       "1  0.016573\n",
       "2 -0.050138\n",
       "3 -0.053433\n",
       "4  0.012658"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.308639</td>\n",
       "      <td>0.198692</td>\n",
       "      <td>0.305213</td>\n",
       "      <td>1.797514</td>\n",
       "      <td>0.334946</td>\n",
       "      <td>0.031710</td>\n",
       "      <td>-0.536027</td>\n",
       "      <td>-0.263113</td>\n",
       "      <td>-4.253643</td>\n",
       "      <td>-0.020784</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.722676</td>\n",
       "      <td>-0.437733</td>\n",
       "      <td>-1.013452</td>\n",
       "      <td>0.253081</td>\n",
       "      <td>-0.353472</td>\n",
       "      <td>-0.323872</td>\n",
       "      <td>-0.217220</td>\n",
       "      <td>-0.316712</td>\n",
       "      <td>0.999226</td>\n",
       "      <td>-2.209761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.035505</td>\n",
       "      <td>0.213459</td>\n",
       "      <td>0.062735</td>\n",
       "      <td>1.834078</td>\n",
       "      <td>3.156576</td>\n",
       "      <td>0.699887</td>\n",
       "      <td>-0.064282</td>\n",
       "      <td>-0.253724</td>\n",
       "      <td>0.420566</td>\n",
       "      <td>-0.213377</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.660679</td>\n",
       "      <td>0.004696</td>\n",
       "      <td>-0.993258</td>\n",
       "      <td>0.014954</td>\n",
       "      <td>-0.357729</td>\n",
       "      <td>-0.324239</td>\n",
       "      <td>-0.126277</td>\n",
       "      <td>0.322742</td>\n",
       "      <td>2.402137</td>\n",
       "      <td>-1.761787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.187051</td>\n",
       "      <td>-0.763254</td>\n",
       "      <td>-0.204179</td>\n",
       "      <td>3.759799</td>\n",
       "      <td>1.749355</td>\n",
       "      <td>-0.243040</td>\n",
       "      <td>-0.248920</td>\n",
       "      <td>-2.957171</td>\n",
       "      <td>-0.189352</td>\n",
       "      <td>-0.672471</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.660679</td>\n",
       "      <td>0.004696</td>\n",
       "      <td>-0.993258</td>\n",
       "      <td>0.014954</td>\n",
       "      <td>-0.357729</td>\n",
       "      <td>-0.324239</td>\n",
       "      <td>-0.126277</td>\n",
       "      <td>0.322742</td>\n",
       "      <td>2.402137</td>\n",
       "      <td>-1.612463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.376448</td>\n",
       "      <td>1.414695</td>\n",
       "      <td>-0.499605</td>\n",
       "      <td>-1.004007</td>\n",
       "      <td>1.686582</td>\n",
       "      <td>-1.117496</td>\n",
       "      <td>-0.922658</td>\n",
       "      <td>-0.285237</td>\n",
       "      <td>-1.314754</td>\n",
       "      <td>-0.541568</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.660679</td>\n",
       "      <td>0.004696</td>\n",
       "      <td>-0.993258</td>\n",
       "      <td>0.014954</td>\n",
       "      <td>-0.357729</td>\n",
       "      <td>-0.324239</td>\n",
       "      <td>-0.126277</td>\n",
       "      <td>0.322742</td>\n",
       "      <td>2.402137</td>\n",
       "      <td>-2.071706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.474105</td>\n",
       "      <td>-0.038048</td>\n",
       "      <td>-1.506717</td>\n",
       "      <td>1.653817</td>\n",
       "      <td>-2.996184</td>\n",
       "      <td>-0.193256</td>\n",
       "      <td>-0.952329</td>\n",
       "      <td>-2.397566</td>\n",
       "      <td>-1.721289</td>\n",
       "      <td>-0.586144</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.660679</td>\n",
       "      <td>0.004696</td>\n",
       "      <td>-0.993258</td>\n",
       "      <td>0.014954</td>\n",
       "      <td>-0.357729</td>\n",
       "      <td>-0.324239</td>\n",
       "      <td>-0.126277</td>\n",
       "      <td>0.322742</td>\n",
       "      <td>2.402137</td>\n",
       "      <td>-2.536584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>-0.250980</td>\n",
       "      <td>0.619440</td>\n",
       "      <td>-2.658982</td>\n",
       "      <td>1.602028</td>\n",
       "      <td>-0.317923</td>\n",
       "      <td>-0.653448</td>\n",
       "      <td>0.026833</td>\n",
       "      <td>-0.602072</td>\n",
       "      <td>-0.912129</td>\n",
       "      <td>0.330256</td>\n",
       "      <td>...</td>\n",
       "      <td>1.773738</td>\n",
       "      <td>-1.328955</td>\n",
       "      <td>1.124573</td>\n",
       "      <td>-0.299808</td>\n",
       "      <td>-0.319334</td>\n",
       "      <td>-0.323954</td>\n",
       "      <td>-0.297840</td>\n",
       "      <td>-1.291119</td>\n",
       "      <td>0.297771</td>\n",
       "      <td>-0.598184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>-0.887390</td>\n",
       "      <td>1.425135</td>\n",
       "      <td>-0.297143</td>\n",
       "      <td>-0.189926</td>\n",
       "      <td>-0.736918</td>\n",
       "      <td>0.065989</td>\n",
       "      <td>0.397249</td>\n",
       "      <td>-0.200602</td>\n",
       "      <td>0.888689</td>\n",
       "      <td>0.132798</td>\n",
       "      <td>...</td>\n",
       "      <td>1.773738</td>\n",
       "      <td>-1.328955</td>\n",
       "      <td>1.124573</td>\n",
       "      <td>-0.299808</td>\n",
       "      <td>-0.319334</td>\n",
       "      <td>-0.323954</td>\n",
       "      <td>-0.297840</td>\n",
       "      <td>-1.291119</td>\n",
       "      <td>0.297771</td>\n",
       "      <td>-0.609453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.824095</td>\n",
       "      <td>0.680492</td>\n",
       "      <td>-2.522328</td>\n",
       "      <td>0.853779</td>\n",
       "      <td>0.391558</td>\n",
       "      <td>0.610593</td>\n",
       "      <td>0.677183</td>\n",
       "      <td>-0.023775</td>\n",
       "      <td>1.006212</td>\n",
       "      <td>0.035450</td>\n",
       "      <td>...</td>\n",
       "      <td>1.928058</td>\n",
       "      <td>-1.227101</td>\n",
       "      <td>2.207406</td>\n",
       "      <td>0.265214</td>\n",
       "      <td>-0.326512</td>\n",
       "      <td>-0.324097</td>\n",
       "      <td>0.707943</td>\n",
       "      <td>-1.321570</td>\n",
       "      <td>-0.052956</td>\n",
       "      <td>-0.522113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>-0.888628</td>\n",
       "      <td>0.791315</td>\n",
       "      <td>8.229035</td>\n",
       "      <td>-0.239071</td>\n",
       "      <td>0.064024</td>\n",
       "      <td>-0.577614</td>\n",
       "      <td>0.544657</td>\n",
       "      <td>1.015291</td>\n",
       "      <td>0.209039</td>\n",
       "      <td>0.789929</td>\n",
       "      <td>...</td>\n",
       "      <td>1.928058</td>\n",
       "      <td>-1.227101</td>\n",
       "      <td>2.207406</td>\n",
       "      <td>0.265214</td>\n",
       "      <td>-0.326512</td>\n",
       "      <td>-0.324097</td>\n",
       "      <td>0.707943</td>\n",
       "      <td>-1.321570</td>\n",
       "      <td>-0.052956</td>\n",
       "      <td>-0.502390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>-0.824413</td>\n",
       "      <td>0.815588</td>\n",
       "      <td>1.611938</td>\n",
       "      <td>-0.719142</td>\n",
       "      <td>0.063704</td>\n",
       "      <td>-0.369217</td>\n",
       "      <td>-0.608386</td>\n",
       "      <td>-0.894917</td>\n",
       "      <td>0.144262</td>\n",
       "      <td>0.697152</td>\n",
       "      <td>...</td>\n",
       "      <td>1.928058</td>\n",
       "      <td>-1.227101</td>\n",
       "      <td>2.207406</td>\n",
       "      <td>0.265214</td>\n",
       "      <td>-0.326512</td>\n",
       "      <td>-0.324097</td>\n",
       "      <td>0.707943</td>\n",
       "      <td>-1.321570</td>\n",
       "      <td>-0.052956</td>\n",
       "      <td>-0.550287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202 rows × 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0    0.308639  0.198692  0.305213  1.797514  0.334946  0.031710 -0.536027   \n",
       "1    0.035505  0.213459  0.062735  1.834078  3.156576  0.699887 -0.064282   \n",
       "2    0.187051 -0.763254 -0.204179  3.759799  1.749355 -0.243040 -0.248920   \n",
       "3    0.376448  1.414695 -0.499605 -1.004007  1.686582 -1.117496 -0.922658   \n",
       "4    0.474105 -0.038048 -1.506717  1.653817 -2.996184 -0.193256 -0.952329   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "197 -0.250980  0.619440 -2.658982  1.602028 -0.317923 -0.653448  0.026833   \n",
       "198 -0.887390  1.425135 -0.297143 -0.189926 -0.736918  0.065989  0.397249   \n",
       "199  0.824095  0.680492 -2.522328  0.853779  0.391558  0.610593  0.677183   \n",
       "200 -0.888628  0.791315  8.229035 -0.239071  0.064024 -0.577614  0.544657   \n",
       "201 -0.824413  0.815588  1.611938 -0.719142  0.063704 -0.369217 -0.608386   \n",
       "\n",
       "           7         8         9   ...        62        63        64  \\\n",
       "0   -0.263113 -4.253643 -0.020784  ... -1.722676 -0.437733 -1.013452   \n",
       "1   -0.253724  0.420566 -0.213377  ... -1.660679  0.004696 -0.993258   \n",
       "2   -2.957171 -0.189352 -0.672471  ... -1.660679  0.004696 -0.993258   \n",
       "3   -0.285237 -1.314754 -0.541568  ... -1.660679  0.004696 -0.993258   \n",
       "4   -2.397566 -1.721289 -0.586144  ... -1.660679  0.004696 -0.993258   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "197 -0.602072 -0.912129  0.330256  ...  1.773738 -1.328955  1.124573   \n",
       "198 -0.200602  0.888689  0.132798  ...  1.773738 -1.328955  1.124573   \n",
       "199 -0.023775  1.006212  0.035450  ...  1.928058 -1.227101  2.207406   \n",
       "200  1.015291  0.209039  0.789929  ...  1.928058 -1.227101  2.207406   \n",
       "201 -0.894917  0.144262  0.697152  ...  1.928058 -1.227101  2.207406   \n",
       "\n",
       "           65        66        67        68        69        70        71  \n",
       "0    0.253081 -0.353472 -0.323872 -0.217220 -0.316712  0.999226 -2.209761  \n",
       "1    0.014954 -0.357729 -0.324239 -0.126277  0.322742  2.402137 -1.761787  \n",
       "2    0.014954 -0.357729 -0.324239 -0.126277  0.322742  2.402137 -1.612463  \n",
       "3    0.014954 -0.357729 -0.324239 -0.126277  0.322742  2.402137 -2.071706  \n",
       "4    0.014954 -0.357729 -0.324239 -0.126277  0.322742  2.402137 -2.536584  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "197 -0.299808 -0.319334 -0.323954 -0.297840 -1.291119  0.297771 -0.598184  \n",
       "198 -0.299808 -0.319334 -0.323954 -0.297840 -1.291119  0.297771 -0.609453  \n",
       "199  0.265214 -0.326512 -0.324097  0.707943 -1.321570 -0.052956 -0.522113  \n",
       "200  0.265214 -0.326512 -0.324097  0.707943 -1.321570 -0.052956 -0.502390  \n",
       "201  0.265214 -0.326512 -0.324097  0.707943 -1.321570 -0.052956 -0.550287  \n",
       "\n",
       "[202 rows x 72 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = skpre.StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "y_train=y_train*1\n",
    "y_test=y_test*1\n",
    "y_train=y_train.astype(int)\n",
    "y_test=y_test.astype(int)   \n",
    "pd.DataFrame(X_train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1) SP_Rebar_pct                   0.038267\n",
      " 2) Inv_Steel_pct                  0.033634\n",
      " 3) FP_Rebar.1                     0.027421\n",
      " 4) CN1YR_pct                      0.026681\n",
      " 5) FB_Mn-Si_pct                   0.025906\n",
      " 6) TV_Rebar_pct                   0.025661\n",
      " 7)  GP_Rebar_pct                  0.023991\n",
      " 8) SP_Coke                        0.022906\n",
      " 9) FB_Coke_pct                    0.022774\n",
      "10) FP_Rebar.1_pct                 0.021769\n",
      "11) HP_Rebar_pct                   0.021163\n",
      "12) FP_Iron_pct                    0.021154\n",
      "13) FP_Rebar                       0.020406\n",
      "14) US1YR_pct                      0.019332\n",
      "15) FP_Rebar_pct                   0.019192\n",
      "16) SP_Iron_pct                    0.019115\n",
      "17) Cost_Rebar_pct                 0.019081\n",
      "18) FB_Coke                        0.018385\n",
      "19) Cost_Rebar                     0.018275\n",
      "20) HP_Rebar                       0.017723\n",
      "21) Inv_Iron_pct                   0.017626\n",
      "22) SP_Coke_pct                    0.017531\n",
      "23) FP_Iron                        0.016980\n",
      "24)  GP_Rebar                      0.016858\n",
      "25) TV_Rebar                       0.015701\n",
      "26) SP_Iron                        0.015681\n",
      "27) US1YR                          0.015648\n",
      "28) Inv_Steel                      0.015378\n",
      "29) CN1YR                          0.015354\n",
      "30) Inv_Iron                       0.015036\n",
      "31) FP_FS_pct                      0.014373\n",
      "32) CHSA                           0.014325\n",
      "33) FP_FS                          0.013999\n",
      "34) M1                             0.013685\n",
      "35) CSA_pct                        0.013622\n",
      "36) CA_REDI                        0.013005\n",
      "37) PPI                            0.012893\n",
      "38) FB_Mn-Si                       0.012715\n",
      "39) SP_Rebar                       0.012708\n",
      "40) PPI_I_mom                      0.011912\n",
      "41) CA_FAI                         0.011397\n",
      "42) rCV_PIron                      0.011219\n",
      "43) rCV_PIron_pct                  0.011129\n",
      "44) CSA                            0.011035\n",
      "45) PPI_pct                        0.010425\n",
      "46) M2                             0.010324\n",
      "47) rCV_Rebar                      0.010170\n",
      "48) CV_Rebar                       0.009814\n",
      "49) OR_Iron                        0.009435\n",
      "50) CA_FAI_pct                     0.009400\n",
      "51) PPI_I_yoy                      0.009317\n",
      "52) CV_PIron                       0.009045\n",
      "53) CV_Steel                       0.008519\n",
      "54) HCA                            0.008160\n",
      "55) OR_Steel_pct                   0.007960\n",
      "56) rCV_Steel                      0.007855\n",
      "57) CV_Iron                        0.007811\n",
      "58) rCV_Rebar_pct                  0.006943\n",
      "59) CV_Iron_pct                    0.006852\n",
      "60) CA_REDI_pct                    0.006705\n",
      "61) rCV_Iron                       0.006465\n",
      "62) CHSA_pct                       0.006359\n",
      "63) HCA_pct                        0.005918\n",
      "64) OR_Steel                       0.005454\n",
      "65) CV_PIron_pct                   0.005359\n",
      "66) CV_Rebar_pct                   0.005106\n",
      "67) OR_Iron_pct                    0.004868\n",
      "68) rCV_Steel_pct                  0.004798\n",
      "69) M1_pct                         0.004357\n",
      "70) rCV_Iron_pct                   0.003892\n",
      "71) M2_pct                         0.003208\n",
      "72) CV_Steel_pct                   0.002832\n"
     ]
    }
   ],
   "source": [
    "#Select Features\n",
    "lables=Input.columns[:]\n",
    "forest = RandomForestClassifier(n_estimators=50,random_state=1)\n",
    "forest.fit(X_train_std, y_train)\n",
    "importances = forest.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "for f in range(X_train_std.shape[1]):\n",
    "    print(\"%2d) %-*s %f\" % (f + 1, 30,lables[indices[f]],importances[indices[f]]))\n",
    "new = SelectFromModel(forest, threshold=0.00999,prefit=True)\n",
    "X_train_std = pd.DataFrame(new.transform(X_train_std))\n",
    "X_test_std   =pd.DataFrame(new.transform(X_test_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrueShortRate= 0.375\n",
      "FalseShortRate= 0.0\n",
      "CaughtShortRate= 0.3\n",
      "TrueLongRate= 0.5\n",
      "FalseLongRate= 0.125\n",
      "CaughtLongRate= 0.2222222222222222\n",
      "LR best score 0.140625\n",
      "LR best para C= 14.3\n",
      "LR return 0.4620109862828252\n",
      "LR Cross validation score [0.45238095 0.14285714 0.33333333 0.23809524 0.38095238]\n"
     ]
    }
   ],
   "source": [
    "#LR model\n",
    "\n",
    "Para_lr_C=[x/10 for x in range(1,400)]\n",
    "lrT_score=-1  #store score\n",
    "lrT_C=0\n",
    "for lrC in Para_lr_C:\n",
    "    lr= LogisticRegression(penalty='l2',\n",
    "                       solver = 'lbfgs',C=lrC,class_weight={0:0.2, 1:0.4,-1:0.4},multi_class='multinomial'\n",
    "                       )\n",
    "    lr.fit(X_train_std, y_train)\n",
    "    b=my_score(lr,X_train_std,y_train,X_test_std,y_test)\n",
    "    if lrT_score<b:\n",
    "        lrT_score=b\n",
    "        lrT_C=lrC\n",
    "\n",
    "lr= LogisticRegression(penalty='l2',\n",
    "                       solver = 'lbfgs',C=lrT_C,class_weight={0:0.2, 1:0.4,-1:0.4},multi_class='multinomial'\n",
    "                       )\n",
    "lr.fit(X_train_std, y_train)\n",
    "lr_pre=pd.DataFrame(lr.predict(X_test_std)) #  trading predicted by LR\n",
    "y_y=y_test.reset_index(drop=1)\n",
    "lr_confusion=pd.DataFrame(confusion_matrix(y_y,lr_pre,labels=[-1,0,1])) #LR's confusion matrix\n",
    "lr_yields=yields(lr_pre,price.iloc[:,1]) #LR's yields\n",
    "lr_accy=accy(lr_yields)\n",
    "metrics(lr_confusion)      \n",
    "my_score(lr,X_train_std,y_train,X_test_std,y_test)        \n",
    "lr_CV_score=cross_val_score(LogisticRegression(penalty='l2',\n",
    "                       solver = 'lbfgs',C=lrT_C,class_weight={0:0.2, 1:0.4,-1:0.4},multi_class='multinomial')\n",
    "                           , X=Input, y=y, cv=tscv,scoring='f1_micro')    \n",
    "print('LR best score',lrT_score)\n",
    "print('LR best para C=',lrT_C)\n",
    "print('LR return',lr_accy.iloc[-1,0]-1)\n",
    "print('LR Cross validation score',lr_CV_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrueShortRate= 0.25\n",
      "FalseShortRate= 0.1875\n",
      "CaughtShortRate= 0.4\n",
      "TrueLongRate= 0.6666666666666666\n",
      "FalseLongRate= 0.3333333333333333\n",
      "CaughtLongRate= 0.1111111111111111\n",
      "SVM best score 0.020833333333333332\n",
      "SVM best para C= 10.9\n",
      "SVM best para gamma= 0.01\n",
      "SVM return 0.23718277473361327\n",
      "SVM Cross validation score [0.33333333 0.66666667 0.54761905 0.57142857 0.47619048]\n"
     ]
    }
   ],
   "source": [
    "#SVM model\n",
    "\n",
    "Para_SVM_C=[x/10 for x in range(1,200,2)]\n",
    "Para_SVM_gamma=[x/100 for x in range(1,2)]\n",
    "SVMT_score=-1  #暂时储存score\n",
    "SVMT_C=0\n",
    "for SVMC in Para_SVM_C:\n",
    "    for SVMgamma in Para_SVM_gamma:\n",
    "        SVM=SVC(kernel='rbf', random_state=0, C=SVMC, gamma=SVMgamma,decision_function_shape='ovr') \n",
    "        SVM.fit(X_train_std, y_train)\n",
    "        SVM_score=my_score(SVM,X_train_std,y_train,X_test_std,y_test)\n",
    "        if SVMT_score<SVM_score:\n",
    "            SVMT_score=SVM_score\n",
    "            SVMT_C=SVMC\n",
    "            SVMT_gamma=SVMgamma\n",
    "SVM=SVC(kernel='rbf', random_state=0, C=SVMT_C, gamma=SVMgamma,decision_function_shape='ovr') \n",
    "SVM.fit(X_train_std, y_train)\n",
    "SVM_pre=pd.DataFrame(SVM.predict(X_test_std)) \n",
    "y_y=y_test.reset_index(drop=1)\n",
    "SVM_confusion=pd.DataFrame(confusion_matrix(y_y,SVM_pre,labels=[-1,0,1])) \n",
    "SVM_yields=yields(SVM_pre,price.iloc[:,1]) \n",
    "SVM_accy=accy(SVM_yields)\n",
    "metrics(SVM_confusion)      \n",
    "my_score(SVM,X_train_std,y_train,X_test_std,y_test)        \n",
    "SVM_CV_score=cross_val_score(SVC(kernel='rbf', random_state=0, C=SVMC, gamma=SVMgamma,decision_function_shape='ovr') \n",
    "                           , X=Input, y=y, cv=tscv,scoring='f1_micro')    \n",
    "print('SVM best score',SVMT_score)\n",
    "print('SVM best para C=',SVMT_C)\n",
    "print('SVM best para gamma=',SVMT_gamma)\n",
    "print('SVM return',SVM_accy.iloc[-1,0]-1)\n",
    "print('SVM Cross validation score',SVM_CV_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrueShortRate= 0.15\n",
      "FalseShortRate= 0.4\n",
      "CaughtShortRate= 0.3\n",
      "TrueLongRate= 0.23076923076923078\n",
      "FalseLongRate= 0.15384615384615385\n",
      "CaughtLongRate= 0.16666666666666666\n",
      "tree best score 0.0\n",
      "tree best para C= 6\n",
      "tree return -0.17862521463422687\n",
      "tree Cross validation score [0.47619048 0.28571429 0.5        0.38095238 0.30952381]\n"
     ]
    }
   ],
   "source": [
    "# DecisionTree model\n",
    "Para_tree_depth=[x for x in range(1,47)]\n",
    "treeT_score=-1  #\n",
    "treeT_depth=0\n",
    "for treedepth in Para_tree_depth:\n",
    "    tree = DecisionTreeClassifier(criterion='gini',max_depth=treedepth)\n",
    "    tree.fit(X_train_std, y_train)\n",
    "    tree_score=my_score(tree,X_train_std,y_train,X_test_std,y_test)\n",
    "    if treeT_score<tree_score:\n",
    "        treeT_score=tree_score\n",
    "        treeT_depth=treedepth\n",
    "tree=DecisionTreeClassifier(criterion='gini',max_depth=treedepth)\n",
    "tree.fit(X_train_std, y_train)\n",
    "tree_pre=pd.DataFrame(tree.predict(X_test_std)) \n",
    "y_y=y_test.reset_index(drop=1)\n",
    "tree_confusion=pd.DataFrame(confusion_matrix(y_y,tree_pre,labels=[-1,0,1])) \n",
    "tree_yields=yields(tree_pre,price.iloc[:,1]) #\n",
    "tree_accy=accy(tree_yields)\n",
    "metrics(tree_confusion)      \n",
    "my_score(tree,X_train_std,y_train,X_test_std,y_test)        \n",
    "tree_CV_score=cross_val_score(DecisionTreeClassifier(criterion='gini',max_depth=treedepth)\n",
    "                          ,X=Input, y=y, cv=tscv,scoring='f1_micro')    \n",
    "print('tree best score',treeT_score)\n",
    "print('tree best para C=',treeT_depth)\n",
    "print('tree return',tree_accy.iloc[-1,0]-1)\n",
    "print('tree Cross validation score',tree_CV_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrueShortRate= 0.25\n",
      "FalseShortRate= 0.0\n",
      "CaughtShortRate= 0.1\n",
      "TrueLongRate= nan\n",
      "FalseLongRate= nan\n",
      "CaughtLongRate= 0.0\n",
      "RF best score -0.0\n",
      "RF best para depth= 12\n",
      "RF return 0.07937434899425755\n",
      "RF Cross validation score [0.47619048 0.28571429 0.5952381  0.42857143 0.33333333]\n"
     ]
    }
   ],
   "source": [
    "#Random forest model\n",
    "Para_RF_n=[x for x in range(1,100)]\n",
    "RFT_score=-1  \n",
    "RFT_n=0\n",
    "for RFn in Para_RF_n:\n",
    "    RF = RandomForestClassifier(n_estimators=RFn,random_state=0, oob_score=1,criterion='gini')\n",
    "    RF.fit(X_train_std, y_train)\n",
    "    RF_score=my_score(RF,X_train_std,y_train,X_test_std,y_test)\n",
    "    if RFT_score<RF_score:\n",
    "        RFT_score=RF_score\n",
    "        RFT_n=RFn\n",
    "RF=RandomForestClassifier(n_estimators=RFn,random_state=0, oob_score=1,criterion='gini')\n",
    "RF.fit(X_train_std, y_train)\n",
    "RF_pre=pd.DataFrame(RF.predict(X_test_std)) \n",
    "y_y=y_test.reset_index(drop=1)\n",
    "RF_confusion=pd.DataFrame(confusion_matrix(y_y,RF_pre,labels=[-1,0,1])) \n",
    "RF_yields=yields(RF_pre,price.iloc[:,1]) \n",
    "RF_accy=accy(RF_yields)\n",
    "metrics(RF_confusion)      \n",
    "my_score(RF,X_train_std,y_train,X_test_std,y_test)        \n",
    "RF_CV_score=cross_val_score(RandomForestClassifier(n_estimators=RFn,random_state=0, oob_score=1,criterion='gini')\n",
    "                          ,X=Input, y=y, cv=tscv,scoring='f1_micro')    \n",
    "print('RF best score',RFT_score)\n",
    "print('RF best para depth=',RFT_n)\n",
    "print('RF return',RF_accy.iloc[-1,0]-1)\n",
    "print('RF Cross validation score',RF_CV_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrueShortRate= 0.24324324324324326\n",
      "FalseShortRate= 0.24324324324324326\n",
      "CaughtShortRate= 0.9\n",
      "TrueLongRate= 0.6\n",
      "FalseLongRate= 0.2\n",
      "CaughtLongRate= 0.16666666666666666\n",
      "gbdt best score 0.017857142857142856\n",
      "gbdt best para n= 101\n",
      "gbdt best para learning rate= 0.8\n",
      "gbdt return 0.11788099907814664\n",
      "gbdt Cross validation score [0.47619048 0.23809524 0.42857143 0.23809524 0.33333333]\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting Decision Tree model\n",
    "Para_gbdt_n=[x for x in range(100,200)]\n",
    "Para_gbdt_l=[x/10 for x in range(1,10)]\n",
    "gbdtT_score=-1 \n",
    "gbdtT_n=0\n",
    "gbdtT_l=0\n",
    "for gbdtn in Para_gbdt_n:\n",
    "    for gbdtl in Para_gbdt_l:\n",
    "        gbdt=GradientBoostingClassifier(n_estimators=gbdtn,learning_rate=gbdtl)\n",
    "        gbdt.fit(X_train_std, y_train)\n",
    "        gbdt_score=my_score(gbdt,X_train_std,y_train,X_test_std,y_test)\n",
    "        if gbdtT_score<gbdt_score:\n",
    "            gbdtT_score=gbdt_score\n",
    "            gbdtT_n=gbdtn\n",
    "            gbdtT_l=gbdtl\n",
    "gbdt=GradientBoostingClassifier(n_estimators=gbdtT_n,learning_rate=gbdtT_l)\n",
    "gbdt.fit(X_train_std, y_train)\n",
    "gbdt_pre=pd.DataFrame(gbdt.predict(X_test_std))\n",
    "y_y=y_test.reset_index(drop=1)\n",
    "gbdt_confusion=pd.DataFrame(confusion_matrix(y_y,gbdt_pre,labels=[-1,0,1]))\n",
    "gbdt_yields=yields(gbdt_pre,price.iloc[:,1]) \n",
    "gbdt_accy=accy(gbdt_yields)\n",
    "metrics(gbdt_confusion)      \n",
    "my_score(gbdt,X_train_std,y_train,X_test_std,y_test)        \n",
    "gbdt_CV_score=cross_val_score(GradientBoostingClassifier(n_estimators=gbdtT_n,learning_rate=gbdtT_l)\n",
    "                          ,X=Input, y=y, cv=tscv,scoring='f1_micro')    \n",
    "print('gbdt best score',gbdtT_score)\n",
    "print('gbdt best para n=',gbdtT_n)\n",
    "print('gbdt best para learning rate=',gbdtT_l)\n",
    "print('gbdt return',gbdt_accy.iloc[-1,0]-1)\n",
    "print('gbdt Cross validation score',gbdt_CV_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
